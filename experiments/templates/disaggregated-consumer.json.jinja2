{
  "name": "Phase {{ phase }}: {{ backend|upper }} Consumer - {{ prompt_tokens }} tokens",
  "description": "{{ backend|upper }} Disaggregated Consumer, {{ prompt_tokens }} tokens, Prefix Cache {{ 'ON' if prefix_cache else 'OFF' }}",
  "variables": {
    "VENV_PATH": "{{ infrastructure.venv_path }}",
    "MODEL": "{{ infrastructure.model }}",
    "PORT": "8200",
    "LOG_FILE": "/tmp/vllm_consumer_{{ backend }}_{{ pattern_id }}.log",
    "PID_FILE": "/tmp/vllm_consumer_{{ backend }}_{{ pattern_id }}.pid",
    "GPU_MEMORY_UTIL": "{{ gpu_memory_utilization }}",
    "MAX_BATCHED_TOKENS": "{{ max_num_batched_tokens }}",
    "MAX_MODEL_LEN": "{{ max_model_len }}"{% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %},
    "TP_SIZE": "{{ infrastructure.tp_size }}"
{% endif %}

  },
  "tasks": [
    {
      "id": "01-cleanup-gpu",
      "name": "Cleanup GPU Memory",
      "description": "Terminate existing vLLM processes",
      "commands": [
        "sudo pkill -9 -f 'vllm.entrypoints' 2>/dev/null || true",
        "sudo pkill -9 -f 'ray::' 2>/dev/null || true",
        "echo '[INFO] Waiting for GPU memory to be released...'",
        "sleep 15",
        "nvidia-smi --query-gpu=index,memory.used --format=csv,noheader || true"
      ]
    },
    {
      "id": "02-start-consumer",
      "name": "Start vLLM Consumer ({{ backend|upper }}, {{ 'Prefix Cache ON' if prefix_cache else 'Prefix Cache OFF' }})",
      "description": "Start Consumer ({{ backend|upper }}, max_model_len={{ max_model_len }})",
      "skip_if": "pgrep -f 'vllm.entrypoints.openai.api_server.*kv_consumer.*--max-model-len {{"{{"}}MAX_MODEL_LEN{{"}}"}}' > /dev/null && curl -s http://localhost:{{"{{"}}PORT{{"}}"}}/health | grep -q ok",
      "commands": [
        "if [ -z \"$NODE2_PRIVATE\" ]; then echo '[ERROR] NODE2_PRIVATE not set'; exit 1; fi",
{% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %}
        "export NCCL_DEBUG=WARN",
        "export NCCL_IB_DISABLE=0",
{% endif %}
{% if backend == 'efa' %}
        "export NIXL_BACKEND=LIBFABRIC",
        "export FI_PROVIDER=efa",
        "export FI_EFA_FORK_SAFE=1",
        "export FI_EFA_USE_DEVICE_RDMA=1",
        "export FI_LOG_LEVEL=warn",
{% elif backend == 'tcp' %}
        "export NIXL_BACKEND=TCP",
{% else %}
        "export NIXL_BACKEND=UCX",
{% endif %}
        "export NIXL_LOG_LEVEL=INFO",
        "export VLLM_NIXL_SIDE_CHANNEL_HOST=$NODE2_PRIVATE",
        "export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1",
        "nohup python3 -m vllm.entrypoints.openai.api_server --model {{"{{"}}MODEL{{"}}"}} --disable-log-requests --trust-remote-code --port {{"{{"}}PORT{{"}}"}} {% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %}--tensor-parallel-size {{ tp_per_node }} {% endif %}--gpu-memory-utilization {{"{{"}}GPU_MEMORY_UTIL{{"}}"}} --max-num-batched-tokens {{"{{"}}MAX_BATCHED_TOKENS{{"}}"}} --enable-chunked-prefill --enforce-eager --max-model-len {{"{{"}}MAX_MODEL_LEN{{"}}"}} {{ '--enable-prefix-caching' if prefix_cache else '--no-enable-prefix-caching' }} --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_consumer\",\"kv_rank\":1,\"kv_parallel_size\":2,\"kv_buffer_device\":\"{% if backend == 'tcp' %}cpu{% else %}cuda{% endif %}\",\"kv_buffer_size\":{{ kv_buffer_size }},\"kv_connector_extra_config\":{\"backends\":[\"{% if backend == 'efa' %}LIBFABRIC{% elif backend == 'tcp' %}TCP{% else %}UCX{% endif %}\"]{% if enable_cross_layers_blocks %},\"enable_cross_layers_blocks\":\"True\"{% endif %}}}' > {{"{{"}}LOG_FILE{{"}}"}} 2>&1 &",
        "echo $! > {{"{{"}}PID_FILE{{"}}"}}",
        "echo '[OK] {{ backend|upper }} Consumer started ({{ 'Prefix Cache ON' if prefix_cache else 'Prefix Cache OFF' }})'"
      ]
    },
    {
      "id": "03-wait-initialization",
      "name": "Wait for Consumer Initialization",
      "description": "Wait for model initialization",
      "commands": [
        "echo '[INFO] Waiting {{ init_wait_seconds }} seconds for initialization...'",
        "sleep {{ init_wait_seconds }}"
      ]
    },
    {
      "id": "04-health-check",
      "name": "Consumer Health Check (OOM Detection)",
      "description": "Verify Consumer server is ready with OOM/error early detection",
      "commands": [
        "HEALTH_OK=false; OOM_DETECTED=false; ERROR_DETECTED=false; HEALTH_TIMEOUT={{ health_check_timeout | default(300) }}; for i in $(seq 1 $HEALTH_TIMEOUT); do HTTP_CODE=$(curl -s -o /dev/null -w '%{http_code}' http://localhost:{{"{{"}}PORT{{"}}"}}/health 2>/dev/null || echo '000'); if [ \"$HTTP_CODE\" = \"200\" ]; then echo \"[OK] Consumer server ready (took $i seconds)\"; HEALTH_OK=true; break; fi; if [ $((i % 10)) -eq 0 ]; then if grep -qE 'torch\\.OutOfMemoryError|CUDA out of memory|torch\\.cuda\\.OutOfMemoryError|Cannot allocate memory|KV cache is needed.*larger than.*available' {{"{{"}}LOG_FILE{{"}}"}} 2>/dev/null; then echo '[ERROR] OOM detected in Consumer logs'; OOM_DETECTED=true; break; fi; if grep -qE 'ValueError.*max seq len|RuntimeError.*CUDA|Traceback \\(most recent call last\\)' {{"{{"}}LOG_FILE{{"}}"}} 2>/dev/null; then echo '[ERROR] Consumer startup error detected'; ERROR_DETECTED=true; break; fi; fi; if [ $((i % 30)) -eq 0 ]; then LAST_LOG=$(tail -1 {{"{{"}}LOG_FILE{{"}}"}} 2>/dev/null || echo 'no log'); echo \"[INFO] Waiting... ($i/$HEALTH_TIMEOUT) - Last log: $LAST_LOG\"; fi; sleep 1; done; if [ \"$OOM_DETECTED\" = \"true\" ]; then echo '[FATAL] VRAM OOM - this pattern requires more GPU memory than available'; echo '--- Last 30 lines of log ---'; tail -30 {{"{{"}}LOG_FILE{{"}}"}};  echo '--- End of log ---'; exit 1; elif [ \"$ERROR_DETECTED\" = \"true\" ]; then echo '[FATAL] Consumer startup error'; echo '--- Last 30 lines of log ---'; tail -30 {{"{{"}}LOG_FILE{{"}}"}};  echo '--- End of log ---'; exit 1; elif [ \"$HEALTH_OK\" = \"false\" ]; then echo \"[ERROR] Consumer health check timeout ($HEALTH_TIMEOUT seconds)\"; echo '--- Last 50 lines of log ---'; tail -50 {{"{{"}}LOG_FILE{{"}}"}};  echo '--- End of log ---'; exit 1; fi"
      ]
    },
    {
      "id": "04b-verify-nixl-backend",
      "name": "Verify NIXL Backend",
      "description": "Verify NIXL backend initialization from logs",
      "commands": [
        "echo '[INFO] Verifying NIXL backend initialization...'",
        "sleep 10",
        "grep -i 'nixl\\|backend\\|LIBFABRIC' {{"{{"}}LOG_FILE{{"}}"}} | tail -20 || echo '[WARNING] No NIXL backend info found in logs'",
        "echo '[OK] Backend verification complete'"
      ]
    }
  ]
}
