{
  "name": "Phase {{ phase }}: Unified - {{ prompt_tokens }} tokens - c={{ concurrency }}",
  "description": "Unified mode, {{ infrastructure.instance_type }} x {{ infrastructure.node_count }} nodes, {{ prompt_tokens }} tokens, c={{ concurrency }}, Prefix Cache {{ 'ON' if prefix_cache else 'OFF' }}",
  "variables": {
    "VENV_PATH": "{{ infrastructure.venv_path }}",
    "MODEL": "{{ infrastructure.model }}",
    "PORT": "8100",
    "BENCHMARK_SCRIPT": "/tmp/benchmark_common.py",
    "OUTPUT_FILE": "/tmp/results/{{ pattern_id }}.json",
    "LOG_FILE": "/tmp/vllm_unified_{{ pattern_id }}.log",
    "PID_FILE": "/tmp/vllm_unified_{{ pattern_id }}.pid",
    "PROMPT_TOKENS": "{{ prompt_tokens }}",
    "MAX_TOKENS": "{{ max_tokens }}",
    "CONCURRENCY": "{{ concurrency }}",
    "WARMUP_ITERATIONS": "{{ warmup_iterations }}",
    "NUM_ITERATIONS": "{{ measurement_iterations }}",
    "GPU_MEMORY_UTIL": "{{ gpu_memory_utilization }}",
    "MAX_BATCHED_TOKENS": "{{ max_num_batched_tokens }}",
    "MAX_MODEL_LEN": "{{ max_model_len }}",
{% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %}
    "TP_SIZE": "{{ infrastructure.tp_size }}",
{% endif %}
    "PHASE": "{{ phase }}"
  },
  "tasks": [
    {
      "id": "01-verify-benchmark-script",
      "name": "Verify Benchmark Script",
      "description": "Verify benchmark script exists",
      "commands": [
        "if [ ! -f {{"{{"}}BENCHMARK_SCRIPT{{"}}"}} ]; then echo '[ERROR] Benchmark script not found: {{"{{"}}BENCHMARK_SCRIPT{{"}}"}}'; exit 1; fi",
        "echo '[OK] Benchmark script found'"
      ]
    },
    {
      "id": "02-cleanup-gpu",
      "name": "Cleanup GPU Memory",
      "description": "Terminate existing vLLM processes and ensure GPU memory is released",
      "commands": [
        "echo '[INFO] Cleaning up GPU memory...'",
        "sudo pkill -9 -f 'vllm.entrypoints' 2>/dev/null || true",
        "sudo pkill -9 -f 'ray::' 2>/dev/null || true",
        "sudo pkill -9 -f 'python' 2>/dev/null || true",
        "sleep 5",
        "nvidia-smi --query-compute-apps=pid --format=csv,noheader 2>/dev/null | while read pid; do [ -n \"$pid\" ] && sudo kill -9 $pid 2>/dev/null || true; done",
        "echo '[INFO] Waiting for GPU memory to be released (max 60 seconds)...'",
        "for i in $(seq 1 12); do MAX_MEM=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits 2>/dev/null | sort -rn | head -1); if [ -z \"$MAX_MEM\" ] || [ \"$MAX_MEM\" -lt 1000 ]; then echo '[OK] GPU memory released'; break; fi; [ $((i % 3)) -eq 0 ] && echo '[INFO] Waiting... GPU memory: '$MAX_MEM' MiB ('$i'/12)'; sleep 5; done",
        "nvidia-smi --query-gpu=index,memory.used --format=csv,noheader || true"
      ]
    },
    {
      "id": "03-start-unified",
      "name": "Start Unified vLLM ({{ 'Prefix Cache ON' if prefix_cache else 'Prefix Cache OFF' }})",
      "description": "Start Unified server (gpu={{ gpu_memory_utilization }}, max_model_len={{ max_model_len }})",
      "skip_if": "pgrep -f 'vllm.entrypoints.openai.api_server.*--max-model-len {{"{{"}}MAX_MODEL_LEN{{"}}"}}' > /dev/null && curl -s http://localhost:{{"{{"}}PORT{{"}}"}}/health | grep -q ok",
      "commands": [
{% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %}
        "export NCCL_DEBUG=INFO",
        "export NCCL_IB_DISABLE=0",
        "export FI_PROVIDER=efa",
        "export FI_EFA_USE_DEVICE_RDMA=1",
{% endif %}
        "nohup python3 -m vllm.entrypoints.openai.api_server --model {{"{{"}}MODEL{{"}}"}} --disable-log-requests --trust-remote-code --port {{"{{"}}PORT{{"}}"}} {% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %}--tensor-parallel-size {{"{{"}}TP_SIZE{{"}}"}} {% endif %}--gpu-memory-utilization {{"{{"}}GPU_MEMORY_UTIL{{"}}"}} --max-num-batched-tokens {{"{{"}}MAX_BATCHED_TOKENS{{"}}"}} --enable-chunked-prefill --enforce-eager --max-model-len {{"{{"}}MAX_MODEL_LEN{{"}}"}} {{ '--enable-prefix-caching' if prefix_cache else '--no-enable-prefix-caching' }} > {{"{{"}}LOG_FILE{{"}}"}} 2>&1 &",
        "echo $! > {{"{{"}}PID_FILE{{"}}"}}",
        "echo '[OK] Unified server started ({{ 'Prefix Cache ON' if prefix_cache else 'Prefix Cache OFF' }})'"
      ]
    },
    {
      "id": "04-wait-initialization",
      "name": "Wait for Initialization",
      "description": "Wait for model initialization",
      "commands": [
        "echo '[INFO] Waiting {{ init_wait_seconds }} seconds for initialization...'",
        "sleep {{ init_wait_seconds }}"
      ]
    },
    {
      "id": "05-health-check",
      "name": "Health Check",
      "description": "Verify vLLM server is ready",
      "commands": [
        "HEALTH_OK=false; for i in $(seq 1 120); do HTTP_CODE=$(curl -s -o /dev/null -w '%{http_code}' http://localhost:{{"{{"}}PORT{{"}}"}}/health 2>/dev/null || echo '000'); if [ \"$HTTP_CODE\" = \"200\" ]; then echo '[OK] vLLM server ready'; HEALTH_OK=true; break; fi; [ $((i % 20)) -eq 0 ] && echo '[INFO] Waiting... ('$i'/120)'; sleep 1; done; if [ \"$HEALTH_OK\" = \"false\" ]; then echo '[ERROR] Health check timeout'; cat {{"{{"}}LOG_FILE{{"}}"}} | tail -50; false; fi"
      ]
    },
{% if measurement_type == 'online' %}
    {
      "id": "06-run-benchmark",
      "name": "Run Benchmark ({{ prompt_tokens }} tokens, c={{ concurrency }})",
      "description": "Run benchmark (warmup={{ warmup_iterations }}, measurement={{ measurement_iterations }}, c={{ concurrency }})",
      "commands": [
        "sudo mkdir -p /tmp/results && sudo chown ubuntu:ubuntu /tmp/results",
        "sudo rm -f {{"{{"}}OUTPUT_FILE{{"}}"}}",
        "sudo -u ubuntu env MLFLOW_EXPERIMENT_TIMESTAMP=\"${MLFLOW_EXPERIMENT_TIMESTAMP:-}\" python3 {{"{{"}}BENCHMARK_SCRIPT{{"}}"}} --measurement-type online --url http://localhost:{{"{{"}}PORT{{"}}"}} --model {{"{{"}}MODEL{{"}}"}} --mode unified --backend none --prompt-tokens {{"{{"}}PROMPT_TOKENS{{"}}"}} --max-tokens {{"{{"}}MAX_TOKENS{{"}}"}} --warmup-iterations {{"{{"}}WARMUP_ITERATIONS{{"}}"}} --num-iterations {{"{{"}}NUM_ITERATIONS{{"}}"}} --concurrency {{"{{"}}CONCURRENCY{{"}}"}} --prefix-cache {{ 'enabled' if prefix_cache else 'disabled' }} --output {{"{{"}}OUTPUT_FILE{{"}}"}} --mlflow-tracking-uri arn:aws:sagemaker:us-east-1:776010787911:mlflow-tracking-server/nixl-efa-mlflow2 --mlflow-experiment-name nixl-efa-phase1 --phase {{"{{"}}PHASE{{"}}"}} --layer '{{ layer_name }}' --priority '{{ layer_priority }}' --tags 'phase={{ phase }}{% if infrastructure.tp_size is defined %},tp_size={{ infrastructure.tp_size }}{% endif %},instance={{ infrastructure.instance_type }}' --mlflow-run-name {{ pattern_id }}-$(date +%Y%m%d-%H%M%S)",
        "echo '[OK] Benchmark complete: {{"{{"}}OUTPUT_FILE{{"}}"}}'"
      ]
    }
{% else %}
    {
      "id": "06-run-benchmark-offline",
      "name": "Run Benchmark (Offline, {{ prompt_tokens }} tokens)",
      "description": "Run offline benchmark (Python API direct)",
      "commands": [
        "sudo mkdir -p /tmp/results && sudo chown ubuntu:ubuntu /tmp/results",
        "sudo rm -f {{"{{"}}OUTPUT_FILE{{"}}"}}",
        "sudo -u ubuntu python3 {{"{{"}}BENCHMARK_SCRIPT{{"}}"}} --measurement-type offline --model {{"{{"}}MODEL{{"}}"}} --prompt-tokens {{"{{"}}PROMPT_TOKENS{{"}}"}} --max-tokens {{"{{"}}MAX_TOKENS{{"}}"}} --num-iterations {{"{{"}}NUM_ITERATIONS{{"}}"}} --prefix-cache {{ 'enabled' if prefix_cache else 'disabled' }} --output {{"{{"}}OUTPUT_FILE{{"}}"}} --mlflow-tracking-uri arn:aws:sagemaker:us-east-1:776010787911:mlflow-tracking-server/nixl-efa-mlflow2 --mlflow-experiment-name nixl-efa-phase1 --phase {{"{{"}}PHASE{{"}}"}} --mlflow-run-name {{ pattern_id }}-$(date +%Y%m%d-%H%M%S)",
        "echo '[OK] Benchmark complete: {{"{{"}}OUTPUT_FILE{{"}}"}}'"
      ]
    }
{% endif %},
    {
      "id": "07-collect-results",
      "name": "Collect Results",
      "description": "Display results JSON",
      "commands": [
        "echo '--- Results Summary ---'",
        "cat {{"{{"}}OUTPUT_FILE{{"}}"}} | head -30 || echo '[INFO] Results file not found'",
        "echo '--- End Results ---'"
      ]
    },
    {
      "id": "99-cleanup-after-measurement",
      "name": "Cleanup After Measurement",
      "description": "Terminate vLLM processes and ensure GPU memory is released",
      "commands": [
        "echo '[INFO] Cleaning up GPU memory...'",
        "sudo pkill -9 -f 'vllm.entrypoints' 2>/dev/null || true",
        "sudo pkill -9 -f 'ray::' 2>/dev/null || true",
        "sudo pkill -9 -f 'python' 2>/dev/null || true",
        "sleep 5",
        "nvidia-smi --query-compute-apps=pid --format=csv,noheader 2>/dev/null | while read pid; do [ -n \"$pid\" ] && sudo kill -9 $pid 2>/dev/null || true; done",
        "echo '[INFO] Waiting for GPU memory to be released (max 60 seconds)...'",
        "for i in $(seq 1 12); do MAX_MEM=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits 2>/dev/null | sort -rn | head -1); if [ -z \"$MAX_MEM\" ] || [ \"$MAX_MEM\" -lt 1000 ]; then echo '[OK] GPU memory released'; break; fi; [ $((i % 3)) -eq 0 ] && echo '[INFO] Waiting... GPU memory: '$MAX_MEM' MiB ('$i'/12)'; sleep 5; done",
        "nvidia-smi --query-gpu=index,memory.used --format=csv,noheader || true",
        "echo '[OK] Cleanup complete'"
      ]
    }
  ]
}
