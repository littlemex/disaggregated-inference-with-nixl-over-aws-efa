{
  "name": "Phase {{ phase }}: {{ backend|upper }} Producer - {{ prompt_tokens }} tokens - c={{ concurrency }}",
  "description": "{{ backend|upper }} Disaggregated Producer, {{ prompt_tokens }} tokens, c={{ concurrency }}, Prefix Cache {{ 'ON' if prefix_cache else 'OFF' }}",
  "variables": {
    "VENV_PATH": "{{ infrastructure.venv_path }}",
    "MODEL": "{{ infrastructure.model }}",
    "PORT": "8100",
    "PROXY_PORT": "8000",
    "CONSUMER_PORT": "8200",
    "BENCHMARK_SCRIPT": "/tmp/benchmark_common.py",
    "PROXY_SCRIPT": "/tmp/disagg_proxy_server.py",
    "OUTPUT_FILE": "/tmp/results/{{ pattern_id }}.json",
    "LOG_FILE": "/tmp/vllm_producer_{{ backend }}_{{ pattern_id }}.log",
    "PID_FILE": "/tmp/vllm_producer_{{ backend }}_{{ pattern_id }}.pid",
    "PROXY_LOG_FILE": "/tmp/proxy_{{ backend }}_{{ pattern_id }}.log",
    "PROXY_PID_FILE": "/tmp/proxy_{{ backend }}_{{ pattern_id }}.pid",
    "PROMPT_TOKENS": "{{ prompt_tokens }}",
    "MAX_TOKENS": "{{ max_tokens }}",
    "CONCURRENCY": "{{ concurrency }}",
    "WARMUP_ITERATIONS": "{{ warmup_iterations }}",
    "NUM_ITERATIONS": "{{ measurement_iterations }}",
    "GPU_MEMORY_UTIL": "{{ gpu_memory_utilization }}",
    "MAX_BATCHED_TOKENS": "{{ max_num_batched_tokens }}",
    "MAX_MODEL_LEN": "{{ max_model_len }}",
{% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %}
    "TP_SIZE": "{{ infrastructure.tp_size }}",
{% endif %}
    "PHASE": "{{ phase }}"
  },
  "tasks": [
    {
      "id": "01-verify-scripts",
      "name": "Verify Scripts",
      "description": "Verify benchmark and proxy scripts exist",
      "commands": [
        "if [ ! -f {{"{{"}}BENCHMARK_SCRIPT{{"}}"}} ]; then echo '[ERROR] Benchmark script not found: {{"{{"}}BENCHMARK_SCRIPT{{"}}"}}'; exit 1; fi",
        "if [ ! -f {{"{{"}}PROXY_SCRIPT{{"}}"}} ]; then echo '[ERROR] Proxy script not found: {{"{{"}}PROXY_SCRIPT{{"}}"}}'; exit 1; fi",
        "echo '[OK] All scripts found'"
      ]
    },
    {
      "id": "02-cleanup-gpu",
      "name": "Cleanup GPU Memory",
      "description": "Terminate existing vLLM processes and ensure GPU memory is released",
      "commands": [
        "echo '[INFO] Cleaning up GPU memory...'",
        "sudo pkill -9 -f 'vllm.entrypoints' 2>/dev/null || true",
        "sudo pkill -9 -f 'ray::' 2>/dev/null || true",
        "sudo pkill -9 -f 'disagg_proxy' 2>/dev/null || true",
        "sudo pkill -9 -f 'python' 2>/dev/null || true",
        "sleep 5",
        "nvidia-smi --query-compute-apps=pid --format=csv,noheader 2>/dev/null | while read pid; do [ -n \"$pid\" ] && sudo kill -9 $pid 2>/dev/null || true; done",
        "echo '[INFO] Waiting for GPU memory to be released (max 60 seconds)...'",
        "for i in $(seq 1 12); do MAX_MEM=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits 2>/dev/null | sort -rn | head -1); if [ -z \"$MAX_MEM\" ] || [ \"$MAX_MEM\" -lt 1000 ]; then echo '[OK] GPU memory released'; break; fi; [ $((i % 3)) -eq 0 ] && echo '[INFO] Waiting... GPU memory: '$MAX_MEM' MiB ('$i'/12)'; sleep 5; done",
        "nvidia-smi --query-gpu=index,memory.used --format=csv,noheader || true"
      ]
    },
    {
      "id": "03-start-producer",
      "name": "Start vLLM Producer ({{ backend|upper }}, {{ 'Prefix Cache ON' if prefix_cache else 'Prefix Cache OFF' }})",
      "description": "Start Producer ({{ backend|upper }}, max_model_len={{ max_model_len }})",
      "skip_if": "pgrep -f 'vllm.entrypoints.openai.api_server.*kv_producer.*--max-model-len {{"{{"}}MAX_MODEL_LEN{{"}}"}}' > /dev/null && curl -s http://localhost:{{"{{"}}PORT{{"}}"}}/health | grep -q ok",
      "commands": [
        "if [ -z \"$NODE1_PRIVATE\" ]; then echo '[ERROR] NODE1_PRIVATE not set'; exit 1; fi",
{% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %}
        "export NCCL_DEBUG=WARN",
        "export NCCL_IB_DISABLE=0",
{% endif %}
{% if backend == 'efa' %}
        "export NIXL_BACKEND=LIBFABRIC",
        "export FI_PROVIDER=efa",
        "export FI_EFA_FORK_SAFE=1",
        "export FI_EFA_USE_DEVICE_RDMA=1",
        "export FI_LOG_LEVEL=warn",
{% elif backend == 'tcp' %}
        "export NIXL_BACKEND=TCP",
{% else %}
        "export NIXL_BACKEND=UCX",
{% endif %}
        "export NIXL_LOG_LEVEL=INFO",
        "export VLLM_NIXL_SIDE_CHANNEL_HOST=$NODE1_PRIVATE",
        "nohup {{"{{"}}VENV_PATH{{"}}"}}/bin/python3 -m vllm.entrypoints.openai.api_server --model {{"{{"}}MODEL{{"}}"}} --disable-log-requests --trust-remote-code --port {{"{{"}}PORT{{"}}"}} {% if infrastructure.tp_size is defined and infrastructure.tp_size > 1 %}--tensor-parallel-size {{ tp_per_node }} {% endif %}--gpu-memory-utilization {{"{{"}}GPU_MEMORY_UTIL{{"}}"}} --max-num-batched-tokens {{"{{"}}MAX_BATCHED_TOKENS{{"}}"}} --enable-chunked-prefill --enforce-eager --max-model-len {{"{{"}}MAX_MODEL_LEN{{"}}"}} {{ '--enable-prefix-caching' if prefix_cache else '--no-enable-prefix-caching' }} --kv-transfer-config '{\"kv_connector\":\"NixlConnector\",\"kv_role\":\"kv_producer\",\"kv_rank\":0,\"kv_parallel_size\":2,\"kv_buffer_device\":\"{% if backend == 'tcp' %}cpu{% else %}cuda{% endif %}\",\"kv_buffer_size\":{{ kv_buffer_size }},\"kv_connector_extra_config\":{\"backends\":[\"{% if backend == 'efa' %}LIBFABRIC{% elif backend == 'tcp' %}TCP{% else %}UCX{% endif %}\"]}}' > {{"{{"}}LOG_FILE{{"}}"}} 2>&1 &",
        "echo $! > {{"{{"}}PID_FILE{{"}}"}}",
        "echo '[OK] {{ backend|upper }} Producer started ({{ 'Prefix Cache ON' if prefix_cache else 'Prefix Cache OFF' }})'"
      ]
    },
    {
      "id": "04-wait-initialization",
      "name": "Wait for Producer Initialization",
      "description": "Wait for model initialization",
      "commands": [
        "echo '[INFO] Waiting {{ init_wait_seconds }} seconds for initialization...'",
        "sleep {{ init_wait_seconds }}"
      ]
    },
    {
      "id": "05-health-check",
      "name": "Producer Health Check",
      "description": "Verify Producer server is ready",
      "commands": [
        "HEALTH_OK=false; for i in $(seq 1 120); do HTTP_CODE=$(curl -s -o /dev/null -w '%{http_code}' http://localhost:{{"{{"}}PORT{{"}}"}}/health 2>/dev/null || echo '000'); if [ \"$HTTP_CODE\" = \"200\" ]; then echo '[OK] Producer server ready'; HEALTH_OK=true; break; fi; [ $((i % 20)) -eq 0 ] && echo '[INFO] Waiting... ('$i'/120)'; sleep 1; done; if [ \"$HEALTH_OK\" = \"false\" ]; then echo '[ERROR] Producer health check timeout'; cat {{"{{"}}LOG_FILE{{"}}"}} | tail -50; false; fi"
      ]
    },
    {
      "id": "05b-verify-nixl-backend",
      "name": "Verify NIXL Backend",
      "description": "Verify NIXL backend initialization from logs",
      "commands": [
        "echo '[INFO] Verifying NIXL backend initialization...'",
        "sleep 10",
        "grep -i 'nixl\\|backend\\|LIBFABRIC' {{"{{"}}LOG_FILE{{"}}"}} | tail -20 || echo '[WARNING] No NIXL backend info found in logs'",
        "echo '[OK] Backend verification complete'"
      ]
    },
    {
      "id": "06-start-proxy",
      "name": "Start Disaggregated Proxy Server",
      "description": "Start Proxy server (Prefill -> Decode request routing)",
      "skip_if": "curl -s http://localhost:{{"{{"}}PROXY_PORT{{"}}"}}/health | grep -q healthy",
      "commands": [
        "if [ -z \"$NODE1_PRIVATE\" ] || [ -z \"$NODE2_PRIVATE\" ]; then echo '[ERROR] NODE1_PRIVATE and NODE2_PRIVATE must be set'; exit 1; fi",
        "nohup {{"{{"}}VENV_PATH{{"}}"}}/bin/python3 {{"{{"}}PROXY_SCRIPT{{"}}"}} --prefill-url http://$NODE1_PRIVATE:{{"{{"}}PORT{{"}}"}} --decode-url http://$NODE2_PRIVATE:{{"{{"}}CONSUMER_PORT{{"}}"}} --port {{"{{"}}PROXY_PORT{{"}}"}} > {{"{{"}}PROXY_LOG_FILE{{"}}"}} 2>&1 &",
        "echo $! > {{"{{"}}PROXY_PID_FILE{{"}}"}}",
        "echo '[OK] Proxy server started on port {{"{{"}}PROXY_PORT{{"}}"}}'",
        "sleep 5"
      ]
    },
    {
      "id": "07-proxy-health-check",
      "name": "Proxy Health Check",
      "description": "Verify Proxy server is ready",
      "commands": [
        "HEALTH_OK=false; for i in $(seq 1 30); do HTTP_CODE=$(curl -s -o /dev/null -w '%{http_code}' http://localhost:{{"{{"}}PROXY_PORT{{"}}"}}/health 2>/dev/null || echo '000'); if [ \"$HTTP_CODE\" = \"200\" ]; then echo '[OK] Proxy server ready'; HEALTH_OK=true; break; fi; [ $((i % 10)) -eq 0 ] && echo '[INFO] Waiting for proxy... ('$i'/30)'; sleep 1; done; if [ \"$HEALTH_OK\" = \"false\" ]; then echo '[ERROR] Proxy health check timeout'; false; fi"
      ]
    },
    {
      "id": "08-run-benchmark",
      "name": "Run Benchmark ({{ backend|upper }}, {{ prompt_tokens }} tokens, c={{ concurrency }})",
      "description": "Run benchmark (Proxy, warmup={{ warmup_iterations }}, measurement={{ measurement_iterations }}, c={{ concurrency }})",
      "commands": [
        "sudo mkdir -p /tmp/results && sudo chown ubuntu:ubuntu /tmp/results",
        "sudo rm -f {{"{{"}}OUTPUT_FILE{{"}}"}}",
        "sudo -u ubuntu env MLFLOW_EXPERIMENT_TIMESTAMP=\"${MLFLOW_EXPERIMENT_TIMESTAMP:-}\" python3 {{"{{"}}BENCHMARK_SCRIPT{{"}}"}} --measurement-type online --url http://localhost:{{"{{"}}PROXY_PORT{{"}}"}} --model {{"{{"}}MODEL{{"}}"}} --mode disaggregated --backend {{ backend }} --prompt-tokens {{"{{"}}PROMPT_TOKENS{{"}}"}} --max-tokens {{"{{"}}MAX_TOKENS{{"}}"}} --warmup-iterations {{"{{"}}WARMUP_ITERATIONS{{"}}"}} --num-iterations {{"{{"}}NUM_ITERATIONS{{"}}"}} --concurrency {{"{{"}}CONCURRENCY{{"}}"}} --prefix-cache {{ 'enabled' if prefix_cache else 'disabled' }} --output {{"{{"}}OUTPUT_FILE{{"}}"}} --mlflow-tracking-uri arn:aws:sagemaker:us-east-1:776010787911:mlflow-tracking-server/nixl-efa-mlflow2 --mlflow-experiment-name nixl-efa-phase1 --phase {{"{{"}}PHASE{{"}}"}} --layer '{{ layer_name }}' --priority '{{ layer_priority }}' --tags 'phase={{ phase }}{% if infrastructure.tp_size is defined %},tp_size={{ infrastructure.tp_size }}{% endif %},instance={{ infrastructure.instance_type }},backend={{ backend }}' --mlflow-run-name {{ pattern_id }}-$(date +%Y%m%d-%H%M%S)",
        "echo '[OK] Benchmark complete: {{"{{"}}OUTPUT_FILE{{"}}"}}'"
      ]
    },
    {
      "id": "09-collect-results",
      "name": "Collect Results",
      "description": "Display results JSON",
      "commands": [
        "echo '--- Results Summary ---'",
        "cat {{"{{"}}OUTPUT_FILE{{"}}"}} | head -30 || echo '[INFO] Results file not found'",
        "echo '--- End Results ---'"
      ]
    },
    {
      "id": "99-cleanup-after-measurement",
      "name": "Cleanup After Measurement",
      "description": "Terminate vLLM Producer and Proxy and ensure GPU memory is released",
      "commands": [
        "echo '[INFO] Cleaning up GPU memory...'",
        "sudo pkill -9 -f 'vllm.entrypoints' 2>/dev/null || true",
        "sudo pkill -9 -f 'ray::' 2>/dev/null || true",
        "sudo pkill -9 -f 'disagg_proxy' 2>/dev/null || true",
        "sudo pkill -9 -f 'python' 2>/dev/null || true",
        "sleep 5",
        "nvidia-smi --query-compute-apps=pid --format=csv,noheader 2>/dev/null | while read pid; do [ -n \"$pid\" ] && sudo kill -9 $pid 2>/dev/null || true; done",
        "echo '[INFO] Waiting for GPU memory to be released (max 60 seconds)...'",
        "for i in $(seq 1 12); do MAX_MEM=$(nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits 2>/dev/null | sort -rn | head -1); if [ -z \"$MAX_MEM\" ] || [ \"$MAX_MEM\" -lt 1000 ]; then echo '[OK] GPU memory released'; break; fi; [ $((i % 3)) -eq 0 ] && echo '[INFO] Waiting... GPU memory: '$MAX_MEM' MiB ('$i'/12)'; sleep 5; done",
        "nvidia-smi --query-gpu=index,memory.used --format=csv,noheader || true",
        "echo '[OK] Cleanup complete'"
      ]
    }
  ]
}
