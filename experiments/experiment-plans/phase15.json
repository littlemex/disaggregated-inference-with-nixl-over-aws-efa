{
  "phase": 15,
  "name": "Distributed Inference Benchmark (32B, Long-Context)",
  "description": "32B model distributed inference on g7e.12xlarge (primary) and g5.12xlarge, TP=4, long-context (4K-100K tokens). Note: YaRN rope_scaling for 50K/100K tokens relies on model config auto-detection.",
  "infrastructure": {
    "instance_type": "g7e.12xlarge",
    "alternative_instance_type": "g5.12xlarge",
    "node_count": 2,
    "model": "Qwen/Qwen2.5-32B-Instruct",
    "vllm_version": "v0.15.1",
    "venv_path": "/opt/dlami/nvme/venv/vllm-0.16",
    "gpu_count_per_node": 4,
    "tp_size": 4,
    "yarn_note": "50K/100K tokens require YaRN rope_scaling. Qwen2.5-32B-Instruct config.json includes rope_scaling settings, so vLLM should auto-detect. Verify with smoke test before full measurement.",
    "multi_instance_note": "Testing on both g7e.12xlarge and g5.12xlarge to identify instance-specific vs instance-independent behaviors."
  },
  "common_settings": {
    "max_tokens": 100,
    "warmup_iterations": 20,
    "measurement_iterations": 30,
    "gpu_memory_utilization": 0.9,
    "max_num_batched_tokens": 32768,
    "max_model_len": 32768,
    "measurement_type": "online"
  },
  "layers": [
    {
      "id": "L0",
      "name": "TP Baseline (4K tokens, Phase 14 calibration)",
      "priority": "P0",
      "description": "Calibration point with Phase 14. 7B vs 32B TTFT scaling at 4K tokens.",
      "patterns": [
        {
          "id": "p15-L0-unified-4k-c1",
          "backend": "unified",
          "prompt_tokens": 4096,
          "concurrency": 1,
          "prefix_cache": true,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L0-unified-4k-c1-nocache",
          "backend": "unified",
          "prompt_tokens": 4096,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768,
          "description": "[HIGH-3 Fix] Fair comparison baseline without Prefix Cache"
        },
        {
          "id": "p15-L0-efa-4k-c1",
          "backend": "efa",
          "prompt_tokens": 4096,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L0-tcp-4k-c1",
          "backend": "tcp",
          "prompt_tokens": 4096,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        }
      ]
    },
    {
      "id": "L1",
      "name": "Unified Token Sweep (20K-100K tokens)",
      "priority": "P0",
      "description": "Long-context token scaling with Prefix Cache ON (Phase 14 compatible).",
      "patterns": [
        {
          "id": "p15-L1-unified-20k-c1",
          "backend": "unified",
          "prompt_tokens": 20000,
          "concurrency": 1,
          "prefix_cache": true,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L1-unified-20k-c4",
          "backend": "unified",
          "prompt_tokens": 20000,
          "concurrency": 4,
          "prefix_cache": true,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L1-unified-50k-c1",
          "backend": "unified",
          "prompt_tokens": 50000,
          "concurrency": 1,
          "prefix_cache": true,
          "max_num_batched_tokens": 65536,
          "max_model_len": 65536
        },
        {
          "id": "p15-L1-unified-50k-c4",
          "backend": "unified",
          "prompt_tokens": 50000,
          "concurrency": 4,
          "prefix_cache": true,
          "max_num_batched_tokens": 65536,
          "max_model_len": 65536
        },
        {
          "id": "p15-L1-unified-100k-c1",
          "backend": "unified",
          "prompt_tokens": 100000,
          "concurrency": 1,
          "prefix_cache": true,
          "max_num_batched_tokens": 131072,
          "max_model_len": 131072
        },
        {
          "id": "p15-L1-unified-100k-c4",
          "backend": "unified",
          "prompt_tokens": 100000,
          "concurrency": 4,
          "prefix_cache": true,
          "max_num_batched_tokens": 131072,
          "max_model_len": 131072
        }
      ]
    },
    {
      "id": "L2",
      "name": "Disaggregated (EFA + TCP, 20K-100K tokens)",
      "priority": "P0",
      "description": "Prefill/Decode separation. Prefix Cache OFF for correct KV-Cache transfer measurement.",
      "patterns": [
        {
          "id": "p15-L2-efa-20k-c1",
          "backend": "efa",
          "prompt_tokens": 20000,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L2-efa-50k-c1",
          "backend": "efa",
          "prompt_tokens": 50000,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 65536,
          "max_model_len": 65536
        },
        {
          "id": "p15-L2-efa-100k-c1",
          "backend": "efa",
          "prompt_tokens": 100000,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 131072,
          "max_model_len": 131072
        },
        {
          "id": "p15-L2-tcp-20k-c1",
          "backend": "tcp",
          "prompt_tokens": 20000,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L2-tcp-50k-c1",
          "backend": "tcp",
          "prompt_tokens": 50000,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 65536,
          "max_model_len": 65536
        },
        {
          "id": "p15-L2-tcp-100k-c1",
          "backend": "tcp",
          "prompt_tokens": 100000,
          "concurrency": 1,
          "prefix_cache": false,
          "max_num_batched_tokens": 131072,
          "max_model_len": 131072
        },
        {
          "id": "p15-L2-efa-20k-c4",
          "backend": "efa",
          "prompt_tokens": 20000,
          "concurrency": 4,
          "prefix_cache": false,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L2-tcp-20k-c4",
          "backend": "tcp",
          "prompt_tokens": 20000,
          "concurrency": 4,
          "prefix_cache": false,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L2-efa-100k-c2",
          "backend": "efa",
          "prompt_tokens": 100000,
          "concurrency": 2,
          "prefix_cache": false,
          "max_num_batched_tokens": 131072,
          "max_model_len": 131072
        }
      ]
    },
    {
      "id": "L3",
      "name": "Concurrency Scale (c=8, c=16)",
      "priority": "P1",
      "description": "High-concurrency bandwidth contention. Unified: Prefix Cache ON, Disagg: Prefix Cache OFF.",
      "patterns": [
        {
          "id": "p15-L3-unified-20k-c8",
          "backend": "unified",
          "prompt_tokens": 20000,
          "concurrency": 8,
          "prefix_cache": true,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L3-unified-20k-c16",
          "backend": "unified",
          "prompt_tokens": 20000,
          "concurrency": 16,
          "prefix_cache": true,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        },
        {
          "id": "p15-L3-efa-20k-c8",
          "backend": "efa",
          "prompt_tokens": 20000,
          "concurrency": 8,
          "prefix_cache": false,
          "max_num_batched_tokens": 32768,
          "max_model_len": 32768
        }
      ]
    }
  ]
}
